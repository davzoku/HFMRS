import loggingimport numpy as npimport pandas as pdfrom sentence_transformers import SentenceTransformerfrom sklearn.impute import SimpleImputerfrom sklearn.metrics.pairwise import cosine_similarityfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import LabelEncoderimport torch, torch_geometricfrom torch_geometric.data import Datafrom torch_geometric.loader import DataLoaderlogging.basicConfig(    format='%(asctime)s : %(levelname)s : %(message)s',    level=logging.INFO,    datefmt='%H:%M:%S')class Datapipeline:    def __init__(self, input_path: str = "../data/processed/data.csv"):        self.df = pd.read_csv(input_path)        self.le = LabelEncoder()        self.imputer = SimpleImputer(strategy="constant", fill_value="missing")        self.categorical_pipeline = Pipeline(            steps=[                ("imputer", SimpleImputer(strategy="constant",                                          fill_value="missing")),                ("encoder", LabelEncoder())            ])        self.device = \            torch.device("cuda" if torch.cuda.is_available() else "cpu")        self.embedder = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)    def main(self):        X, y = self._extract_target_features()        modelId_mapping = self._map_modelId(y)        y = self._encode_target_feature(y)        downloads_mapping = self._map_downloads(X)        X = self._drop_columns(X)        X = self._impute_missing_values(X)        X = self._embed_categorical_features(X)        data = self._create_graph(X, y)        # train_loader, test_loader = self._convert_to_dataloader(data)                # return train_loader, test_loader, modelId_mapping, downloads_mapping        return data, modelId_mapping, downloads_mapping        @staticmethod    def compute_cosine_similarity(X: pd.DataFrame):        cosine_similarity_matrix = cosine_similarity(X)        return cosine_similarity_matrix    def _extract_target_features(self):        X = self.df.drop(['modelId'], axis=1)        y = self.df['modelId']        return X, y    def _drop_columns(self,                       X: pd.DataFrame,                       columns: list = ['downloads', 'soup']):        """Drop column(s) in Pandas DataFrame.        :param columns: List of column(s) to drop. Defaults to ['soup'].        :type columns: list, optional        """        X.drop(labels=columns, axis=1, inplace=True)        logging.info(f"Dropped columns - {columns}")        return X        def _map_modelId(self, y: pd.Series):        modeId_to_node_idx = {i: model_id for i, model_id in enumerate(y)}                return modeId_to_node_idx        def _map_downloads(self, X: pd.DataFrame):        downloads_to_node_idx = \            {i: downloads for i, downloads in enumerate(X['downloads'])}                return downloads_to_node_idx        def _impute_missing_values(self, X: pd.DataFrame):        X_imputed = \            pd.DataFrame(self.imputer.fit_transform(X), columns=X.columns)                logging.info("Imputed missing values")        return X_imputed    def _encode_target_feature(self, y: pd.Series, target: str = 'modelId'):        y_encoded = self.le.fit_transform(y)        self.y_original = self.le.inverse_transform(y_encoded)        logging.info(f"Encoded target feature - {target}")        return y_encoded        def _embed_categorical_features(self, X: pd.DataFrame):        cat_features = X.select_dtypes(include=['object']).columns        embeddings = []                for feature in cat_features:            feature_embedding = \                self.embedder.encode(X[feature], convert_to_tensor=True)            embeddings.append(feature_embedding)                    corpus = torch.cat(embeddings, dim=1)                logging.info('Embedded features')        return corpus      def _create_graph(self, X, y, test_size=0.2):        # Compute the cosine similarity matrix        cosine_similarity_matrix = cosine_similarity(X)        # print(type(cosine_similarity_matrix))        # cosine_similarity_matrix = cosine_similarity(X).astype(np.float32)                # Create edge_index and edge_weight tensors        mask = np.triu(np.ones_like(cosine_similarity_matrix), k=1).astype(bool)        edge_weight = cosine_similarity_matrix[mask]        edge_index = np.vstack(np.where(mask)).astype(int)                # Create the train and test masks        n_samples = X.shape[0]        n_test_samples = int(test_size * n_samples)        test_mask = torch.zeros(n_samples, dtype=torch.bool)        test_mask[:n_test_samples] = True        train_mask = ~test_mask                # Create the graph data object        data = Data(            x=torch.as_tensor(X, dtype=torch.float32, device=self.device),             edge_index=torch.as_tensor(edge_index,                                        dtype=torch.long,                                        device=self.device),             edge_weight=torch.as_tensor(edge_weight,                                         dtype=torch.float32,                                         device=self.device),             y=torch.as_tensor(y, dtype=torch.long, device=self.device),             train_mask=train_mask,             test_mask=test_mask            )                return data        # def convert_to_dataloader(self,     #                         data: torch_geometric.data.Data,     #                         batch_size: int = 32):    #     train_mask, test_mask = data.train_mask, data.test_mask            #     train_indices = torch.where(train_mask)[0]    #     test_indices = torch.where(test_mask)[0]            #     # train_subset = torch.utils.data.Subset(data, train_indices)    #     # test_subset = torch.utils.data.Subset(data, test_indices)            #     train_data = data[train_mask]    #     test_data = data[test_mask]            #     train_loader = \    #         DataLoader(train_data, batch_size=batch_size, shuffle=True)    #     test_loader = DataLoader(test_data, batch_size=batch_size)            #     return train_loader, test_loader                # if __name__ == "__main__":#     datapipeline = Datapipeline()    # data = datapipeline.main()        # train_loader, test_loader, model_id_to_node_idx, downloads_to_node_idx = \    #     datapipeline.main()    # X, y = datapipeline.main()    # data = datapipeline.create_graph(X, y)    # train_loader, test_loader = datapipeline.convert_to_dataloader(data)    